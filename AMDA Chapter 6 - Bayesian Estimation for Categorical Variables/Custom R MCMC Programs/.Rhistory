Yind <- rep(0, N)
Yind[is.na(Y)] <- 1
# initial imputations fill in the mean
Y[Yind == 1] <- mean(Y, na.rm = T)
# initialize algorithmic features
set.seed(90291)
iterations <- 11000
burnin <- 1000
# store parameter estimates
betas <- matrix(0, ncol = ncol(X), nrow = iterations)
sigma2e <- matrix(1, iterations)
R2 <- matrix(1, iterations)
stanbeta <- matrix(1, iterations)
# gibbs sampler
for(t in 2:iterations){
# print iteration history
if(t %% 500 == 0){print(paste0("Iteration = " , t))}
# regression coefficients, conditional on residual variance and missing values
betahat <- solve(crossprod(X,X)) %*% crossprod(X,Y)
covbeta <- solve(crossprod(X,X)) * sigma2e[t-1]
betas[t,] <- rmvnorm(1, mean = betahat, sigma = covbeta)
# sample variance, conditional on coefficients and missing values
df <- N / 2
sumofsq <- sum((Y - X %*% betas[t,])^2) / 2
sigma2e[t] <- 1 / rgamma(1, df, rate = sumofsq)
# compute R^2 and standardized B1
R2[t] <- betas[t,2]^2 * var(X[,2]) / var(Y)
stanbeta[t] <- betas[t,2] * sd(X[,2]) / sd(Y)
# impute Y conditional on analysis model parameters
Yhat <- X %*% betas[t,]
varY <- sigma2e[t]
imps <- rnorm(N, Yhat, sqrt(varY))
Y[Yind == 1] <- imps[Yind == 1]
}
# summarize posterior distributions
summary <- matrix(0, nrow = 5, ncol = 5)
rownames(summary) <- c("B0", "B1", "res. var.", "B1(std.)", "R-sq")
colnames(summary) <- c("Mean", "StdDev", "2.5%", "50%", "97.5%")
# summarize posterior distributiona
params <- cbind(betas, sigma2e, stanbeta, R2)
for(p in 1:nrow(summary)){
summary[p,1] <- mean(params[(burnin+1):iterations,p])
summary[p,2] <- sd(params[(burnin+1):iterations,p])
summary[p,3:5] <- quantile(params[(burnin+1):iterations,p], c(.025, .50, .975))
plot(density(params[(burnin+1):iterations,p]), main = rownames(summary)[p], xlab = "Parameter Value")
}
print(paste0("Posterior Distribution Summary from ", iterations - burnin, " Iterations:"))
print(round(summary, 3))
# load packages
library(fdir)
library(mvtnorm)
# set working directory to this script's location
fdir::set()
# read data
dat <- read.table("employee.dat", na.strings = "999")
names(dat) = c("employee", "team", "turnover", "male", "empower", "lmx", "jobsat", "climate", "cohesion")
N <- nrow(dat)
# select variables
X <- dat$lmx
Y <- dat$empower
# load packages
library(fdir)
library(mvtnorm)
# set working directory to this script's location
fdir::set()
# read data
dat <- read.table("employee.dat", na.strings = "999")
names(dat) = c("employee", "team", "turnover", "male", "empower", "lmx", "jobsat", "climate", "cohesion")
N <- nrow(dat)
# select variables
X <- dat$lmx
Y <- dat$empower
# missing data indicators
Yind <- rep(0, N)
Yind[is.na(Y)] <- 1
Xind <- rep(0, N)
Xind[is.na(X)] <- 1
# initial imputations fill in the mean
Y[Yind == 1] <- mean(Y, na.rm = T)
X[Xind == 1] <- mean(X, na.rm = T)
# assemble X matrix
X <- cbind(1, X)
# initialize algorithmic features
set.seed(90291)
iterations <- 11000
burnin <- 1000
# store parameter estimates
betas <- matrix(0, ncol = ncol(X), nrow = iterations)
sigma2e <- matrix(1, iterations)
R2 <- matrix(1, iterations)
stanbeta <- matrix(1, iterations)
muX <- matrix(0, iterations)
sigma2X <- matrix(1, iterations)
t <- 2
# gibbs sampler
for(t in 2:iterations){
# print iteration history
if(t %% 500 == 0){print(paste0("Iteration = " , t))}
# regression coefficients, conditional on residual variance and missing values
betahat <- solve(crossprod(X,X)) %*% crossprod(X,Y)
covbeta <- solve(crossprod(X,X)) * sigma2e[t-1]
betas[t,] <- rmvnorm(1, mean = betahat, sigma = covbeta)
# sample Y variance, conditional on coefficients and missing values
df <- N / 2
sumofsq <- sum((Y - X %*% betas[t,])^2) / 2
sigma2e[t] <- 1 / rgamma(1, df, rate = sumofsq)
# compute R^2 and standardized B1
R2[t] <- betas[t,2]^2 * var(X[,2]) / var(Y)
stanbeta[t] <- betas[t,2] * sd(X[,2]) / sd(Y)
# sample X mean, conditional on missing values
Xbar <- mean(X[,2])
VarXbar <- sigma2X[t-1] / N
muX[t] <- rnorm(1, mean = Xbar, sd = sqrt(VarXbar))
# sample X variance, conditional on mean and missing values
a <- N / 2
b <- sum((X[,2] - muX[t])^2) / 2
sigma2X[t] <- 1 / rgamma(1, a, rate = b)
# impute Y conditional on analysis model parameters
Yhat <- X %*% betas[t,]
varY <- sigma2e[t]
imps <- rnorm(length(Y), Yhat, sqrt(varY))
Y[Yind == 1] <- imps[Yind == 1]
# impute X conditional on two sets of model parameters
Xhat <- (sigma2X[t] * betas[t,2] * (Y - betas[t,1]) + sigma2e[t] * muX[t]) / (betas[t,2]^2 * sigma2X[t] + sigma2e[t])
varX <- (sigma2e[t] * sigma2X[t]) / (betas[t,2]^2 * sigma2X[t] + sigma2e[t])
imps <- rnorm(N, mean = Xhat, sd = sqrt(varX))
X[Xind == 1, 2] <- imps[Xind == 1]
}
# summarize posterior distributions
summary <- matrix(0, nrow = 7, ncol = 5)
rownames(summary) <- c("B0", "B1", "res. var.", "B1(std.)","R-sq", "X mean", "X var")
colnames(summary) <- c("Mean", "StdDev", "2.5%", "50%", "97.5%")
# summarize posterior distributiona
params <- cbind(betas, sigma2e, stanbeta, R2, muX, sigma2X)
for(p in 1:nrow(summary)){
summary[p,1] <- mean(params[(burnin+1):iterations,p])
summary[p,2] <- sd(params[(burnin+1):iterations,p])
summary[p,3:5] <- quantile(params[(burnin+1):iterations,p], c(.025, .50, .975))
plot(density(params[(burnin+1):iterations,p]), main = rownames(summary)[p], xlab = "Parameter Value")
}
print(paste0("Posterior Distribution Summary from ", iterations - burnin, " Iterations:"))
print(round(summary, 3))
# load packages
library(fdir)
library(mvtnorm)
library(MCMCpack)
# load packages
library(fdir)
library(mvtnorm)
library(MCMCpack)
install.packages("MCMCpack")
# load packages
library(fdir)
library(mvtnorm)
library(MCMCpack)
library(truncnorm)
# set working directory to this script's location
fdir::set()
# read data
dat <- read.table("employeecomplete.dat")
names(dat) = c("employee", "team", "turnover", "male", "empower", "lmx", "jobsat", "climate", "teamperf")
N <- nrow(dat)
# select variables
X <- dat$lmx
Y <- dat$turnover
# assemble X matrix
X <- cbind(1, X)
# initialize algorithmic features
set.seed(90291)
iterations <- 11000
burnin <- 1000
# initialize Ystar
Ystar <- rep(0, length(Y))
# store parameter estimates
betas <- matrix(0, ncol = ncol(X), nrow = iterations)
sigma2e <- 1
R2 <- matrix(1, iterations)
stanbeta <- matrix(1, iterations)
# gibbs sampler
t <- 2
for(t in 2:iterations){
# print iteration history
if(t %% 500 == 0){print(paste0("Iteration = " , t))}
# sample latent scores
Ystarhat <- X %*% betas[t-1,]
Ystar[Y == 0] <- rtruncnorm(length(Y[Y == 0]), a = -Inf, b = 0, mean = Ystarhat[Y == 0], sd = 1)
Ystar[Y == 1] <- rtruncnorm(length(Y[Y == 1]), a = 0, b = Inf, mean = Ystarhat[Y == 1], sd = 1)
# regression coefficients, conditional on residual variance and latent scores
betahat <- solve(crossprod(X,X)) %*% crossprod(X,Ystar)
covbeta <- solve(crossprod(X,X)) * sigma2e
betas[t,] <- rmvnorm(1, mean = betahat, sigma = covbeta)
# compute R^2 and standardized B1
R2[t] <- betas[t,2]^2 * var(X[,2]) / (betas[t,2]^2 * var(X[,2]) + 1)
stanbeta[t] <- betas[t,2] * sd(X[,2]) / sqrt(betas[t,2]^2 * var(X[,2]) + 1)
}
# summarize posterior distributions
summary <- matrix(0, nrow = 5, ncol = 5)
rownames(summary) <- c("B0", "B1", "res. var.", "B1(std.)","R-sq")
colnames(summary) <- c("Mean", "StdDev", "2.5%", "50%", "97.5%")
# summarize posterior distributions
params <- cbind(betas, sigma2e, stanbeta, R2)
for(p in 1:nrow(summary)){
summary[p,1] <- mean(params[(burnin+1):iterations,p])
summary[p,2] <- sd(params[(burnin+1):iterations,p])
summary[p,3:5] <- quantile(params[(burnin+1):iterations,p], c(.025, .50, .975))
plot(density(params[(burnin+1):iterations,p]), main = rownames(summary)[p], xlab = "Parameter Value")
}
print(paste0("Posterior Distribution Summary from ", iterations - burnin, " Iterations:"))
print(round(summary, 3))
# load packages
library(fdir)
library(mvtnorm)
library(MCMCpack)
library(truncnorm)
# set working directory to this script's location
fdir::set()
# read data
dat <- read.table("employee.dat", na.strings = "999")
names(dat) = c("employee", "team", "turnover", "male", "empower", "lmx", "jobsat", "climate", "cohesion")
N <- nrow(dat)
# select variables
X <- dat$lmx
Y <- dat$turnover
# missing data indicators
Yind <- rep(0, N)
Yind[is.na(Y)] <- 1
Xind <- rep(0, N)
Xind[is.na(X)] <- 1
# initial X imputations at the mean
X[Xind == 1] <- mean(X, na.rm = T)
# initial discrete imputes w random draw from binomial distribution w p = .50
Yinitial <- rbinom(length(Y), 1, .50)
Y[Yind == 1] <- Yinitial[Yind == 1]
# initial latent scores
pY1 <- mean(Y)
Ystarmean <- -1 * qnorm(pY1, lower.tail = F)
Ystar <- rep(0, length(Y))
Ystar[Y == 0] <- rtruncnorm(length(Y[Y == 0]), a = -Inf, b = 0, mean = Ystarmean, sd = 1)
Ystar[Y == 1] <- rtruncnorm(length(Y[Y == 1]), a = 0, b = Inf, mean = Ystarmean, sd = 1)
# inspect Ystar values
plot(density(Ystar))
abline(v = 0)
# assemble X matrix
X <- cbind(1, X)
# initialize algorithmic features
set.seed(90291)
iterations <- 11000
burnin <- 1000
# store parameter estimates
betas <- matrix(0, ncol = ncol(X), nrow = iterations)
sigma2e <- 1
R2 <- matrix(1, iterations)
stanbeta <- matrix(1, iterations)
muX <- matrix(0, iterations)
sigma2X <- matrix(1, iterations)
# gibbs sampler
t <- 2
for(t in 2:iterations){
# print iteration history
if(t %% 500 == 0){print(paste0("Iteration = " , t))}
# sample latent scores for complete cases
Ystarhat <- X %*% betas[t-1,]
Ystar[Y == 0 & Yind == 0] <- rtruncnorm(length(Y[Y == 0 & Yind == 0]), a = -Inf, b = 0, mean = Ystarhat[Y == 0 & Yind == 0], sd = 1)
Ystar[Y == 1 & Yind == 0] <- rtruncnorm(length(Y[Y == 1 & Yind == 0]), a = 0, b = Inf, mean = Ystarhat[Y == 1 & Yind == 0], sd = 1)
# sample regression coefficients, conditional on current data
betahat <- solve(crossprod(X,X)) %*% crossprod(X,Ystar)
covbeta <- solve(crossprod(X,X)) * sigma2e
betas[t,] <- rmvnorm(1, mean = betahat, sigma = covbeta)
# compute R^2 and standardized B1
R2[t] <- betas[t,2]^2 * var(X[,2]) / (betas[t,2]^2 * var(X[,2]) + 1)
stanbeta[t] <- betas[t,2] * sd(X[,2]) / sqrt(betas[t,2]^2 * var(X[,2]) + 1)
# sample X mean, conditional on variance and current data
Xbar <- mean(X[,2])
VarXbar <- sigma2X[t-1] / N
muX[t] <- rnorm(1, mean = Xbar, sd = sqrt(VarXbar))
# sample X variance, conditional on mean and current data
a <- N / 2
b <- sum((X[,2] - muX[t])^2) / 2
sigma2X[t] <- 1 / rgamma(1, a, rate = b)
# impute missing latent Ys, conditional on regression model
Ystarhat <- X %*% betas[t,]
varY <- sigma2e
imps <- rnorm(length(Y), Ystarhat, sqrt(varY))
Ystar[Yind == 1] <- imps[Yind == 1]
# categorize imputes (if we want to save them)
Y[Ystar > 0] <- 1
Y[Ystar <= 0] <- 0
# impute X conditional on two sets of model parameters
Xhat <- (sigma2X[t] * betas[t,2] * (Ystar - betas[t,1]) + sigma2e * muX[t]) / (betas[t,2]^2 * sigma2X[t] + sigma2e)
varX <- (sigma2e * sigma2X[t]) / (betas[t,2]^2 * sigma2X[t] + sigma2e)
imps <- rnorm(N, mean = Xhat, sd = sqrt(varX))
X[Xind == 1, 2] <- imps[Xind == 1]
}
# summarize posterior distributions
summary <- matrix(0, nrow = 7, ncol = 5)
rownames(summary) <- c("B0", "B1", "res. var.", "B1(std.)","R-sq", "X mean", "X var")
colnames(summary) <- c("Mean", "StdDev", "2.5%", "50%", "97.5%")
# summarize posterior distributiona
params <- cbind(betas, sigma2e, stanbeta, R2, muX, sigma2X)
for(p in 1:nrow(summary)){
summary[p,1] <- mean(params[(burnin+1):iterations,p])
summary[p,2] <- sd(params[(burnin+1):iterations,p])
summary[p,3:5] <- quantile(params[(burnin+1):iterations,p], c(.025, .50, .975))
plot(density(params[(burnin+1):iterations,p]), main = rownames(summary)[p], xlab = "Parameter Value")
}
print(paste0("Posterior Distribution Summary from ", iterations - burnin, " Iterations:"))
print(round(summary, 3))
sigma2e
# load packages
library(fdir)
library(mvtnorm)
library(MCMCpack)
library(truncnorm)
# set working directory to this script's location
fdir::set()
# read data
dat <- read.table("/employeecomplete.dat")
# load packages
library(fdir)
library(mvtnorm)
library(MCMCpack)
library(truncnorm)
# set working directory to this script's location
fdir::set()
# read data
dat <- read.table("employeecomplete.dat")
names(dat) = c("employee", "team", "turnover", "male", "empower", "lmx", "jobsat", "climate", "cohesion")
N <- nrow(dat)
# select variables
Y <- dat$jobsat
X <- dat$lmx
# assemble X matrix
X <- cbind(1, X)
# initialize algorithmic features
iterations <- 11000
burnin <- 1000
proposalsd0 <- .035
proposalvar_multiplier <- 1
# initial threshold estimates
highcat <- as.numeric(max(names(table(Y))))
lowcat <- as.numeric(min(names(table(Y))))
proportions <- table(Y)/sum(table(Y))
tau0 <- rep(0,highcat - 1)
for(c in 1:(highcat-1)){tau0[c] <- qnorm(sum(proportions[1:c]),0,1)}
tau0 <- c(-Inf, tau0 - tau0[1], Inf)
# store parameter estimates
Ystar <- rep(0, length(Y))
betas <- matrix(0, ncol = 2, nrow = iterations); betas[,1] <- 2
thresholds <- matrix(rep(tau0, each = iterations), nrow = iterations)
threshaccept <- matrix(0, nrow = iterations, ncol = 1)
# gibbs sampler
for(t in 2:iterations){
if(t%%100 == 0){
print(paste0("Iteration = " , t))
}
# tune MH acceptance rates
tunecheck <- 100
if(t%%tunecheck == 0 & t <= burnin){
if(mean(threshaccept[(t-(tunecheck-1)):t]) > .50){proposalvar_multiplier <- proposalvar_multiplier * 1.25}
if(mean(threshaccept[(t-(tunecheck-1)):t]) < .25){proposalvar_multiplier <- proposalvar_multiplier * .75}
print(paste0("Iteration = ", t, " | Acceptance rate for Last 50 Adjustment Reps = ", round(mean(threshaccept[(t-(tunecheck-1)):t]),2)))
}
# draw candidate thresholds from proposal distribution
thresh_old <- thresholds[t-1,]
sdMH <- proposalsd0 * proposalvar_multiplier
thresh_new <- thresh_old
for(c in 3:highcat){
thresh_new[c] <- rtruncnorm(1, a = thresh_new[c-1], b = thresh_old[c+1], mean = thresh_old[c], sd = sdMH)
}
# compute predicted latent scores
xBi <- X %*% betas[t-1,]
# compute likeihood part of IR
ll <- rep(0, length(Y))
for(i in 1:N){
ll[i] <- (pnorm(thresh_new[Y[i]+1], xBi[i]) - pnorm(thresh_new[Y[i]], xBi[i])) / (pnorm(thresh_old[Y[i]+1], xBi[i]) - pnorm(thresh_old[Y[i]],xBi[i]))
}
# compute normalization offset part of IR
offset <- rep(1, length(thresh_new))
for(c in 3:highcat){
offset[c] <- (pnorm(thresh_old[c+1],thresh_old[c],sdMH) - pnorm(thresh_new[c-1],thresh_old[c],sdMH)) /
(pnorm(thresh_new[c+1],thresh_new[c],sdMH) - pnorm(thresh_old[c-1],thresh_new[c],sdMH))
}
# importance ratio
ir <- prod(ll) * prod(offset)
u <- runif(1)
if(ir > u){
threshaccept[t] <- 1
thresholds[t,] <- thresh_new
} else{
thresholds[t,] <- thresh_old
}
# sample latent scores
for(c in lowcat:highcat){
Ystar[Y == c] <- rtruncnorm(length(Y[Y == c]), a = thresholds[t,c], b = thresholds[t,c+1], mean = xBi[Y == c], sd = 1)
}
# sample analysis model coefficients
betahat <- solve(crossprod(X,X)) %*% crossprod(X,Ystar)
covbeta <- solve(crossprod(X,X))
betas[t,] <- rmvnorm(1, mean = betahat, sigma = covbeta)
}
# summarize posterior distributions
regparams <- matrix(0, nrow = 2, ncol = 5)
rownames(regparams) <- c("B0", "B1")
colnames(regparams) <- c("Mean", "StdDev", "2.5%", "50%", "97.5%")
# summarize posterior distribution of the slope
regparams[2,1] <- mean(betas[(burnin+1):iterations,2])
regparams[2,2] <- sd(betas[(burnin+1):iterations,2])
regparams[2,3:5] <- quantile(betas[(burnin+1):iterations,2], c(.025, .50, .975))
# summarize posterior distributions
threshparams <- matrix(0, nrow = 6, ncol = 5)
rownames(threshparams) <- c("tau1","tau2","tau3","tau4","tau5","tau6")
colnames(threshparams) <- c("Mean", "StdDev", "2.5%", "50%", "97.5%")
# summarize posterior distribution of the mean
threshparams[,1] <- colMeans(thresholds[(burnin+1):iterations,2:(highcat)])
threshparams[,2] <- sqrt(diag(cov(thresholds[(burnin+1):iterations,2:highcat])))
threshparams[1,3:5] <- quantile(thresholds[(burnin+1):iterations,2], c(.025, .50, .975))
threshparams[2,3:5] <- quantile(thresholds[(burnin+1):iterations,3], c(.025, .50, .975))
threshparams[3,3:5] <- quantile(thresholds[(burnin+1):iterations,4], c(.025, .50, .975))
threshparams[4,3:5] <- quantile(thresholds[(burnin+1):iterations,5], c(.025, .50, .975))
threshparams[5,3:5] <- quantile(thresholds[(burnin+1):iterations,6], c(.025, .50, .975))
threshparams[6,3:5] <- quantile(thresholds[(burnin+1):iterations,7], c(.025, .50, .975))
print(paste0("Posterior Distribution Summary from ", iterations - burnin, " Iterations:"))
summary <- rbind(regparams,threshparams)
print(round(summary,3))
# kernel density plots of posteriors
params <- cbind(betas,thresholds[,2:7])
for(p in 1:nrow(summary)){
plot(density(params[(burnin+1):iterations,p]), main = rownames(summary)[p], xlab = "Parameter Value")
}
# load packages
library(fdir)
library(mvtnorm)
library(MCMCpack)
library(truncnorm)
# set working directory to this script's location
fdir::set()
# read data
dat <- read.table("employeecomplete.dat")
names(dat) = c("employee", "team", "turnover", "male", "empower", "lmx", "jobsat", "climate", "cohesion")
N <- nrow(dat)
# select variables
Y <- dat$jobsat
X <- dat$lmx
# assemble X matrix
X <- cbind(1, X)
# initialize algorithmic features
iterations <- 11000
burnin <- 3000
proposalsd0 <- .035
proposalvar_multiplier <- 1
# initial threshold estimates
highcat <- as.numeric(max(names(table(Y))))
lowcat <- as.numeric(min(names(table(Y))))
proportions <- table(Y)/sum(table(Y))
tau0 <- rep(0,highcat - 1)
for(c in 1:(highcat-1)){tau0[c] <- qnorm(sum(proportions[1:c]),0,1)}
tau0 <- c(-Inf, tau0 - tau0[1], Inf)
# store parameter estimates
Ystar <- rep(0, length(Y))
betas <- matrix(0, ncol = 2, nrow = iterations); betas[,1] <- 2
thresholds <- matrix(rep(tau0, each = iterations), nrow = iterations)
threshaccept <- matrix(0, nrow = iterations, ncol = 1)
# gibbs sampler
for(t in 2:iterations){
if(t%%100 == 0){
print(paste0("Iteration = " , t))
}
# tune MH acceptance rates
tunecheck <- 100
if(t%%tunecheck == 0 & t <= burnin){
if(mean(threshaccept[(t-(tunecheck-1)):t]) > .50){proposalvar_multiplier <- proposalvar_multiplier * 1.25}
if(mean(threshaccept[(t-(tunecheck-1)):t]) < .25){proposalvar_multiplier <- proposalvar_multiplier * .75}
print(paste0("Iteration = ", t, " | Acceptance rate for Last 50 Adjustment Reps = ", round(mean(threshaccept[(t-(tunecheck-1)):t]),2)))
}
# draw candidate thresholds from proposal distribution
thresh_old <- thresholds[t-1,]
sdMH <- proposalsd0 * proposalvar_multiplier
thresh_new <- thresh_old
for(c in 3:highcat){
thresh_new[c] <- rtruncnorm(1, a = thresh_new[c-1], b = thresh_old[c+1], mean = thresh_old[c], sd = sdMH)
}
# compute predicted latent scores
xBi <- X %*% betas[t-1,]
# compute likeihood part of IR
ll <- rep(0, length(Y))
for(i in 1:N){
ll[i] <- (pnorm(thresh_new[Y[i]+1], xBi[i]) - pnorm(thresh_new[Y[i]], xBi[i])) / (pnorm(thresh_old[Y[i]+1], xBi[i]) - pnorm(thresh_old[Y[i]],xBi[i]))
}
# compute normalization offset part of IR
offset <- rep(1, length(thresh_new))
for(c in 3:highcat){
offset[c] <- (pnorm(thresh_old[c+1],thresh_old[c],sdMH) - pnorm(thresh_new[c-1],thresh_old[c],sdMH)) /
(pnorm(thresh_new[c+1],thresh_new[c],sdMH) - pnorm(thresh_old[c-1],thresh_new[c],sdMH))
}
# importance ratio
ir <- prod(ll) * prod(offset)
u <- runif(1)
if(ir > u){
threshaccept[t] <- 1
thresholds[t,] <- thresh_new
} else{
thresholds[t,] <- thresh_old
}
# sample latent scores
for(c in lowcat:highcat){
Ystar[Y == c] <- rtruncnorm(length(Y[Y == c]), a = thresholds[t,c], b = thresholds[t,c+1], mean = xBi[Y == c], sd = 1)
}
# sample analysis model coefficients
betahat <- solve(crossprod(X,X)) %*% crossprod(X,Ystar)
covbeta <- solve(crossprod(X,X))
betas[t,] <- rmvnorm(1, mean = betahat, sigma = covbeta)
}
